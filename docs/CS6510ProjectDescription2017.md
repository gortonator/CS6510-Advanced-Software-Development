


igortn&commat;northeastern.edu<br>
<img src="https://raw.githubusercontent.com/gortonator/CS6510-Advanced-Software-Development/master/img/CS6510.png"> <br>

#**Advanced Software Development** 
##**Spring 2017**
**Professor:** Ian Gorton, igortn@northeastern.edu


###**Project Overview**

We will work with Pacific Northwest National Laboratory ([PNNL](www.pnl.gov))  to build a scientific data management and analysis system using the Amazon Web Services EC2 Cloud. The system will provide a range of capabilities for scientists to store, share and analyze data and publish results that can be made both public, and securely shared within a project community. 

The project will focus on creating the following capabilities: 

 1. A web platform that allows a scientific project to create a shared
    workspace, user accounts, and a set of Web pages that can be used to
    store, access and visualize data sets. 
    
 2. A RESTful API that supports integration with 3rd party data analysis
        tools and existing data stores
    
 3. A collection of data analysis, mining and visualization algorithms
        that a scientist can configure to execute automatically when a new
        data set is uploaded.
 1. A flexible metadata store so that individual projects can tag
        important features in their data to support powerful search and
        discovery tools.

The project will utilize specific capabilities in AWS such as S3 data storage, lambda services, dynamodb, elastic search, message queueing and workflow services, and elastic load balancing. Agile development techniques will be used to deliver working versions of the system through the course of the semester. Candidate development languages are likely Python, Node.js/Javascript and Java. 

###**Course Organization** 

The project will work with software engineers and atmospheric scientists at PNNL. The initial use case will be the Atmosphere to Electrons ([A2e](https://a2e.energy.gov/)) project which is  improving wind plant performance  to achieve substantial reductions in the cost of wind energy production.  

Students will form small teams based on locations (Boston, Seattle) and system components and be allocated responsibility for designing and building a specific system component. The development will follow agile techniques with deliverables, defined by each team, every 3-4 weeks. Teams will be responsible for designing interfaces and APIs to facilitate cross-component communications and publishing these for use by other teams.   

A more detailed project description is given below.


----------


#**Taking Velo to the Clouds** 

###**Executive Summary** 

[Velo](http://www.pnl.gov/computing/velo/index.stm) has been developed at PNNL over a number of years as a domain-agnostic, data management, simulation and analysis platform. Velo is deployed both within PNNL and externally, encompassing disciplines such as carbon sequestration, subsurface simulation, geothermal modeling, and electron microscopy. Currently, each new separate Velo deployment requires the associated science project to deploy and manage its own server environment, which can be an impediment to adoption due to associated upfront and ongoing costs. Further, the existing server architecture has scalability limits in handling big data volumes and velocities, and becomes overwhelmed when concurrent users multiply from tens to hundreds. In response to these problems, this white paper describes how Velo can be transformed into a cloud-hosted Platform-as-a-Service (PaaS) technology. This would enable science teams to subscribe to a Velo service that can be quickly customized for their project. Costs would be minimized through a pay-as-you-go model, and the underlying cloud services would provide the necessary scaling capabilities to accommodate any size project. 

###**Aims and Objectives** 
Capturing, managing and curating scientific data from high fidelity instruments and simulations is placing an ever-increasing burden on scientific research teams. This is a burden that will only increase as instrument technology advances (e.g. The Square Kilometre Array) and simulations routinely move to exascale. 

Velo has been created at PNNL to provide a flexible and powerful platform for science. Velo is intentionally designed to be highly configurable so that heterogeneous data formats and simulations can be integrated without any significant new development. This makes it applicable to potentially any scientific domain. Evidence of this lies in successful deployments for DOE-EM's ASCEM project (http://esd1.lbl.gov/research/projects/ascem/), EPA's Class VI Underground Injection Control Program (https://www.epa.gov/uic/final-class-vi-guidance-documents),  and DOE-EERE's Geothermal Technology Office (http://energy.gov/eere/geothermal/geothermal-energy-us-department-energy). 

Since Velo was conceived, commercial and on-premise cloud technologies have become economically viable for storing massive amounts of data at low cost. Cloud platforms such as Amazon Web Services (AWS) can also provide computational resources 'on demand' to cost-effectively perform simulations and analyses, as these cloud systems support a pay-for-use model. This is a major factor in reducing upfront capital investment along with setup and maintenance costs, especially for large scale government funded projects. 

Providing Velo to the scientific community as a highly configurable, massively scalable Platform-as-a-Service (PaaS) technology would eliminate major barriers to adoption and provide significant cost reductions.  We envisage a Velo PaaS that would enable scientific user communities to: 

 - Create a shared, secure workspace on a cloud platform for user access

 - Specify project data organization, data types and metadata 
   
 - Specify data sources (e.g. instruments, model outputs) and any
   pre-processing and data quality checking required 
   
 - Specify a toolbox of analysis and simulation codes that are available
   for the user community

The Velo PaaS would provide services that make all of the above possible without programming, making the service accessible to scientists without comprehensive software development skills. Velo would leverage the underlying cloud services to dynamically allocate the necessary storage, install analysis and simulation tools based on dynamically loadable virtual environments, and generate personal and shared workspaces for each user in the project community.  

###**Towards  a Velo PaaS** 

We propose to migrate Velo to become a cloud-based platform, initially running on Amazon Web Services. The platform would maintain interface compatibility with the existing Velo implementation so that all current deployments and users can be supported downstream. The existing Velo server based on Alfresco would be replaced by equivalent capabilities that exploit AWS's massively scalable data storage and compute capabilities. For example: 

**Metadata storage:** Initially AWS DynamoDB or equivalent can be used for project metadata storage. Metadata representing all project artifacts (e.g. data, simulation setup, results, reports, papers) can be indexed and held in DynamoDB  at low cost. I 

**Data storage:** Large scale data sets can be stored in Amazon's S3 storage tier. This has essentially unlimited scalability.  S3 data sets that are seldom accessed can be moved to Amazon's Glacier storage, which provides long term archiving at very low cost. 

**Semantic search service:** Customized search capabilities for a Velo project site can be provided using CloudSearch. This provides powerful, programmable search capabilities that can index and operate on structured data and plain text.  

**Domain-specific Analysis Tools:** We will exploit AWS's ability to dynamically load and execute domain-specific analysis tools so that each Velo site can install a custom set of capabilities for its users. A simple Web-based UI will provide users with the ability to select tools and data sets, and Velo will orchestrate the execution of the task, informing the user when the results are complete. 

**Fast File Transfer:** We will integrate Velo with the Globus Online file transfer service to enable high speed data transport to the Velo PaaS from computational centers. If low bandwidth transfer is acceptable AWS's Snowball batch service can be utilized at considerably lower cost. 

We believe that a robust and scalable implementation of these services can form the core of a Velo Paas that can achieve broad user acceptance in the scientific community.




























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































